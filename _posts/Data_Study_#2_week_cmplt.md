

```python
import pandas as pd
import numpy as np

alldfs = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]
print(alldfs)
```

    []
    


```python
import os
print (os.getcwd()) #현재 디렉토리의
os.chdir('./Desktop/bigdata/')
```

    C:\Users\kyuri Im
    


```python
#1.데이터 불러오기 (BUYTEST.csv) 
df1 = pd.read_csv('./Data_Study_19.05.19~/20190526 과제/20190526 과제/BUYTEST.csv')
```


```python
#2.데이터 검토 및 변수 별 기초통계 확인(명목형, 수치형)
df1.info()
cate_cols = [col for col in df1.columns if df1[col].dtype == 'object']
num_cols =[col for col in df1.columns if df1[col].dtype in ['float', 'int64']]
print(cate_cols)
print(num_cols)
print(len(cate_cols)+len(num_cols))

df1[num_cols].describe()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 10000 entries, 0 to 9999
    Data columns (total 26 columns):
    ID          10000 non-null object
    RESPOND     10000 non-null int64
    AGE         10000 non-null object
    INCOME      10000 non-null object
    SEX         9766 non-null object
    MARRIED     10000 non-null object
    FICO        10000 non-null object
    OWNHOME     10000 non-null object
    LOC         10000 non-null object
    CLIMATE     10000 non-null int64
    BUY6        10000 non-null int64
    BUY12       10000 non-null int64
    BUY18       10000 non-null int64
    BUY24       10000 non-null int64
    ORGSRC      9479 non-null object
    DISCBUY     10000 non-null int64
    RETURN24    10000 non-null int64
    COA6        10000 non-null int64
    C1          10000 non-null int64
    C2          10000 non-null int64
    C3          10000 non-null int64
    C4          10000 non-null int64
    C5          10000 non-null int64
    C6          10000 non-null int64
    C7          10000 non-null int64
    PURCHTOT    10000 non-null int64
    dtypes: int64(17), object(9)
    memory usage: 2.0+ MB
    ['ID', 'AGE', 'INCOME', 'SEX', 'MARRIED', 'FICO', 'OWNHOME', 'LOC', 'ORGSRC']
    ['RESPOND', 'CLIMATE', 'BUY6', 'BUY12', 'BUY18', 'BUY24', 'DISCBUY', 'RETURN24', 'COA6', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'PURCHTOT']
    26
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RESPOND</th>
      <th>CLIMATE</th>
      <th>BUY6</th>
      <th>BUY12</th>
      <th>BUY18</th>
      <th>BUY24</th>
      <th>DISCBUY</th>
      <th>RETURN24</th>
      <th>COA6</th>
      <th>C1</th>
      <th>C2</th>
      <th>C3</th>
      <th>C4</th>
      <th>C5</th>
      <th>C6</th>
      <th>C7</th>
      <th>PURCHTOT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.00000</td>
      <td>10000.000000</td>
      <td>10000.00000</td>
      <td>10000.00000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.076700</td>
      <td>20.343000</td>
      <td>0.128300</td>
      <td>0.209900</td>
      <td>0.347100</td>
      <td>254.045100</td>
      <td>0.270000</td>
      <td>0.070900</td>
      <td>0.028900</td>
      <td>0.155300</td>
      <td>1.01220</td>
      <td>0.563500</td>
      <td>0.53730</td>
      <td>0.35400</td>
      <td>3.491600</td>
      <td>0.174500</td>
      <td>6.288400</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.266128</td>
      <td>6.108689</td>
      <td>0.346195</td>
      <td>0.436648</td>
      <td>0.570661</td>
      <td>153.422061</td>
      <td>0.443982</td>
      <td>0.256671</td>
      <td>0.167534</td>
      <td>1.488087</td>
      <td>6.28347</td>
      <td>4.240303</td>
      <td>4.54936</td>
      <td>2.92789</td>
      <td>15.493222</td>
      <td>1.863437</td>
      <td>27.276327</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>10.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>60.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>20.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>149.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>20.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>214.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.000000</td>
      <td>20.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>312.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>30.000000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>1253.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>46.000000</td>
      <td>115.00000</td>
      <td>127.000000</td>
      <td>125.00000</td>
      <td>90.00000</td>
      <td>249.000000</td>
      <td>62.000000</td>
      <td>446.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
df1[cate_cols].describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>AGE</th>
      <th>INCOME</th>
      <th>SEX</th>
      <th>MARRIED</th>
      <th>FICO</th>
      <th>OWNHOME</th>
      <th>LOC</th>
      <th>ORGSRC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>10000</td>
      <td>10000</td>
      <td>10000</td>
      <td>9766</td>
      <td>10000</td>
      <td>10000</td>
      <td>10000</td>
      <td>10000</td>
      <td>9479</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>10000</td>
      <td>59</td>
      <td>85</td>
      <td>2</td>
      <td>3</td>
      <td>196</td>
      <td>3</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>top</th>
      <td>109419955</td>
      <td>44</td>
      <td>60</td>
      <td>M</td>
      <td>1</td>
      <td>700</td>
      <td>0</td>
      <td>E</td>
      <td>O</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1</td>
      <td>503</td>
      <td>297</td>
      <td>5277</td>
      <td>5705</td>
      <td>165</td>
      <td>6503</td>
      <td>2261</td>
      <td>2017</td>
    </tr>
  </tbody>
</table>
</div>




```python
#SEX, ORGSRC 결측값 있음
#ORGSRC, INCOME, AGE 연속형으로 재분류
num_cols += ['AGE', 'INCOME','ORGSRC']
cate_cols = [col for col in df1.columns if col not in num_cols]
```


```python
df1['INCOME'].unique()
```




    array(['67', '72', '70', '56', '66', '48', '49', '64', '65', '$null$',
           '58', '40', '57', '33', '17', '71', '25', '51', '52', '38', '54',
           '29', '63', '61', '19', '35', '31', '20', '24', '45', '73', '30',
           '53', '41', '37', '43', '60', '44', '27', '34', '21', '69', '55',
           '59', '18', '68', '39', '74', '50', '82', '28', '42', '114', '76',
           '46', '36', '62', '84', '16', '47', '81', '32', '75', '79', '77',
           '78', '26', '83', '22', '15', '86', '23', '92', '87', '105', '88',
           '107', '90', '93', '99', '85', '101', '80', '89', '96'],
          dtype=object)




```python
#3.데이터 시각화 : 범주형 변수와 수치형 변수 간의 관계를 보기 위해 박스 플롯 작성 CHAS(찰스강 인접여부: F, T)와 MEDV(집값의 중앙값, 단위 $1000)
import matplotlib.pyplot as plt
import seaborn as sns
```


```python
#4.결측치 처리 : 수치형->평균값, 범주형->최빈값
#SEX, ORGSRC 결측값 있음
df1['SEX'].fillna(df1['SEX'].mode()[0], inplace=True)
df1['ORGSRC'].fillna(df1['ORGSRC'].mode()[0], inplace=True)

df1.isnull().sum()

df1['AGE'].unique()
df1['AGE'] = df1['AGE'].replace('$null$', round(pd.to_numeric(df1['AGE'],errors='coarce').mean()))
df1['AGE'].unique()

###
#INCOME, FICO, MARRIED  ,  OWNHOME        '$null$'  있음
for col in df1.columns:
    print(col,'\n',df1[col].unique(),'\n','-'*50) 
    


for col in ['INCOME', 'FICO']:
    df1[col].replace('$null$', round(pd.to_numeric(df1[col], errors='coarce').mean()), inplace=True)


for col in ['OWNHOME', 'MARRIED']:
    df1[col].replace('$null$', df1[col].mode()[0],inplace=True)

for col in df1.columns:
    print(col,'\n',df1[col].unique(),'\n','-'*50) 


#age income
for col in cate_cols:
    if col in ['AGE','INCOME','FICO']:
        df1[col] = df1[col].astype(np.int8)
```

    ID 
     ['1371057' '2093270' '2783726' ... '996739474' '998468785' '998947462'] 
     --------------------------------------------------
    RESPOND 
     [0 1] 
     --------------------------------------------------
    AGE 
     ['71' '53' '45' '32' '35' '43' '39' '66' 45 '52' '29' '48' '67' '44' '59'
     '47' '49' '36' '23' '58' '50' '63' '55' '54' '42' '46' '60' '56' '37'
     '41' '34' '38' '33' '27' '68' '31' '19' '64' '40' '51' '28' '57' '18'
     '26' '21' '61' '20' '62' '30' '65' '69' '25' '72' '73' '70' '22' '24'
     '75' '74'] 
     --------------------------------------------------
    INCOME 
     ['67' '72' '70' '56' '66' '48' '49' '64' '65' '$null$' '58' '40' '57' '33'
     '17' '71' '25' '51' '52' '38' '54' '29' '63' '61' '19' '35' '31' '20'
     '24' '45' '73' '30' '53' '41' '37' '43' '60' '44' '27' '34' '21' '69'
     '55' '59' '18' '68' '39' '74' '50' '82' '28' '42' '114' '76' '46' '36'
     '62' '84' '16' '47' '81' '32' '75' '79' '77' '78' '26' '83' '22' '15'
     '86' '23' '92' '87' '105' '88' '107' '90' '93' '99' '85' '101' '80' '89'
     '96'] 
     --------------------------------------------------
    SEX 
     ['M' 'F'] 
     --------------------------------------------------
    MARRIED 
     ['1' '0' '$null$'] 
     --------------------------------------------------
    FICO 
     ['719' '751' '725' '684' '651' '691' '694' '659' '692' '707' '705' '693'
     '698' '713' '703' '680' '610' '670' '686' '733' '714' '732' '706' '635'
     '687' '654' '720' '688' '646' '734' '717' '676' '673' '709' '727' '724'
     '657' '678' '738' '729' '744' '675' '701' '681' '643' '658' '650' '740'
     '672' '690' '$null$' '708' '685' '742' '677' '716' '704' '697' '695'
     '710' '715' '739' '668' '632' '737' '748' '718' '722' '666' '721' '769'
     '642' '683' '669' '712' '649' '679' '770' '674' '689' '749' '735' '730'
     '728' '616' '700' '777' '665' '699' '741' '664' '633' '648' '711' '682'
     '637' '661' '798' '647' '745' '702' '645' '757' '726' '696' '746' '667'
     '640' '619' '639' '662' '631' '750' '652' '660' '736' '671' '638' '625'
     '663' '723' '615' '630' '656' '731' '763' '603' '758' '644' '653' '655'
     '622' '776' '760' '623' '617' '636' '634' '761' '762' '628' '607' '743'
     '754' '747' '767' '627' '606' '780' '755' '765' '624' '593' '759' '611'
     '768' '641' '618' '756' '621' '752' '753' '614' '596' '598' '766' '601'
     '629' '793' '626' '784' '773' '781' '604' '597' '602' '771' '605' '764'
     '779' '620' '589' '772' '599' '613' '591' '794' '594' '585' '775' '608'
     '612' '800' '782' '791' '577'] 
     --------------------------------------------------
    OWNHOME 
     ['0' '1' '$null$'] 
     --------------------------------------------------
    LOC 
     ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'] 
     --------------------------------------------------
    CLIMATE 
     [10 20 30] 
     --------------------------------------------------
    BUY6 
     [1 0 2] 
     --------------------------------------------------
    BUY12 
     [1 0 2 3] 
     --------------------------------------------------
    BUY18 
     [1 0 2 3] 
     --------------------------------------------------
    BUY24 
     [ 318   83  265  448  161  250  194  446  214  198  216  118  226  145
      494  485  258  230  189  412  499  107  228  162   97  283  337  349
      180   76  256  351  128  513   66  495  113  223  325  246  404  147
      247  644  119  201  243  422  143  191  464  165  144  259  179  169
      109  424  206  146  237   64   92  359  612  188  235  417  410   84
      330  164   98   87  137  354  541  726  199  207  120  187  195  236
      151  339  310  204   65  124  425  173  316  434  211  104  205  572
      114  234   78  127  208  398  341  219   75  405  231  227  110  450
      429  177  220  311   63  595  160   62  182  154  209  475  409  476
      455  108  239   81   95  121  126  213  232   96  253  163  386  251
      728  101  303   71  297  132   88  493  245  203  249   99  184  133
      487  241  218  700  326  200  210  197  225   74  252  175  150  319
      431  185  459  365  171  157  142   93  186  481  254  131  248  331
      229   82  720  159  115  238  620   89  407  103  396  138  282  302
      122  167  172  281  178  183  355  217  222  174  190  347  531  135
      421   67  443  148   77  156  152  334  123  463  141   85  299  202
      192  435  300   60  224   69  278  884  280  442  324  193  102   73
      290  437  263  385  567  168  212  515  460  451  356   94  155  401
      509  242  136   68   72  221  491  403  130  125  105  314  320  390
      470  158  149  257  321  170  377  129  112  176  483  166 1253  255
      261  284  333  139  556  275  420  360  373  591  602  387  504  244
       70  106  116  262  181  111  478  416  438  370  503  345  497  766
      399  279  489  117  433  346  306   80  240  379  338  472   90  196
      465  233  348  289  484  332  488  524  593  140  550  635   61  292
      376  153  419  313  389  384  288  371  492  364  134  308  327  471
      294  519  445  468  469  393  352  636  452  388  394  453  505   79
      312  268  606  276  941  100  436  456  565  362  562  739  415  328
      672  583  512  653  458  363  517  427  423  508  391  270  350  368
      291   91  215  507  729  756  426  260  634   86  599  500  554  498
      502  757  413  266  441  506  721  552  397  516  755  382  361  466
      820  704  406  690  428  601  418  477  395  746  486  375  329  286
      315  699  411  317  430  367  378  335  444  691  694  697  277  307
      722  578  273  701  340  626  344  447  304  462  343  366  540  696
      771  457  614  440  380  482  769  369  272  661  772  402  305  685
      473  322  323  751  545  358  353  600  555  589  414  267  790  480
      543  905  264  548  521  569  536  782  432  768  269  703  479  534
      566  518  838  621  686  285  730  539  525  392  847  301  496  792
      532  732  563  542  274  551  342  559  538  736  408  520  374  400
      295  535  695  684  588  986  622  725  747  514  647  474  381  727
      598  718  658  309  383  760  454  461  611  714  439  659  616  577
      372  298  640  510  287  523  336  683  557  675  296  529  271  608
      776  638  743 1026  579  698  627  357  811  490  560  568  511  677
      762  449  934  780  625  293  775  916  549  716  765  533  609  581
      617  742  501  774  576  666  922  607  467  909  741  715  682  645
      652  657  748  753  954  709  624  723  530  706  547  573  674  587
      527  788  798  575  630  662  860  605  597  561  604 1238  998  613
      680  926  892  667  564  843  977  596  528  927  787  877  885  584
     1012  670  603  669  952  930  781  574 1112  802  660  982  807  544
      580  869  917  553  649 1008  939  637  623  650  676  713  901  702
      761  724  663  646  984  758  975  858  619  968  628  793  770  794
      980  641  618  814  983  522 1204  991  655  878  754  678  671  914
     1010  639  801  933 1018  673  633  783  615  871  537  844  985  651
      656  919  654  863  832 1013  719  900  546  594 1017  915  935  668
      570  759  840  693  586] 
     --------------------------------------------------
    ORGSRC 
     ['O' 'R' 'D' 'C' 'U' 'P' 'I'] 
     --------------------------------------------------
    DISCBUY 
     [1 0] 
     --------------------------------------------------
    RETURN24 
     [0 1] 
     --------------------------------------------------
    COA6 
     [0 1] 
     --------------------------------------------------
    C1 
     [ 0 10  7  1 13  6  3  9 24 18  8 11  4 15 46 22 16  5  2 25 17 12 30 14
     29] 
     --------------------------------------------------
    C2 
     [  0  11  23   9  32  40  13  29   8  16  49  57  17  42  21  34  31  26
      73  37  33  10  24  12  53  39  22  20  28  47  18  19  38  80  35  14
      50  27  30  36  55  25  46  41  66  60  44  59  54  71  43  52  64  63
      15  92 101   5  45 115 111  93   4   2  61  67  51  65  77  90 112] 
     --------------------------------------------------
    C3 
     [  0  53   9  42  32  25  29  21  10  14   8  15  36  34  17  11  18  16
      30  13  31  41  39  12   5  35  19  68  22  20  54  28  38  27  24  23
      43  40  51  46  61  50  58  79  26   6  45  57 127  85  47  37  83] 
     --------------------------------------------------
    C4 
     [  0  40   3  82   9  31  41  26  14  27  11  13  28  17  37  21  19  35
      51  22  44  16   8  10  39   6  56  72  62  20  55  15  50  32  71  29
      12  38  23  24  18  53  36  33  30   7  49  25  85  54  73  48  34  59
      63 125  64 102  57] 
     --------------------------------------------------
    C5 
     [ 0 41  8 20 21 16 13 11 26 23  9 18 10 15 17 19 14 42 12 44 25 27 31 35
     29 33 37 55  3 38 54 90 24 56 22 58] 
     --------------------------------------------------
    C6 
     [  0  64  66  85  30  18  32  52  80  11  16  13  51  31 112   9  34  39
      45  72  19  35  28  17   8  77  21  25  22  23  38  47  65  12  55 188
      26  42  49  62  41  54  36 249  40 121  70  56 100  69  83 107 106  15
      29  37  27  57  48  61  50 117  58  14  24  68  82 128  73  76  93  79
      96  20 199  10 120  87 150  33  71  86 102  67  89 110 126  97  59  43
     136  46  81  44  78  53 103 118  84 129  74  75 158  88 135  63 104  99
      60 116   7  92 191 111 114 220  98 162 176 152 130  94 168 163 190 123
     153 105 124 101 139 115 149 147] 
     --------------------------------------------------
    C7 
     [ 0 11 19  8 10 20 15 13 38  9 12 16 14  1 17 41 22 32 62 30 21 24 25 56
     23 26 18] 
     --------------------------------------------------
    PURCHTOT 
     [  0  86 210 128  30  18  32   3 196   9  88  11 109  85  52 117  71 122
      23  34  39  81 102  60  64  65  26 186  21  25  51  49  41 116  58  12
      53 214  87  89 152  50 140 138  84 346 125  78  22 142  99  36  66 134
      79 114  45 115 229 147  67  37  44  61  42  70  83 150   6  93  31  33
     178 126  76  14  35  46  80  68  91 153  54  95  17  94  19 105  16 124
      27 108 151  62  20  63  43 113 163  47 288  10 118 160 127   5 171  90
      40  56 104  69  24 167 184 137 256 131  57 121 168  29  92 136 107  82
       1 208 106 129  72  96 174  97  48 123 145  77 148 173 139  13  74  75
     130 100 119 206 155 103 201 141 143 157   7 180 217 199  59  55 158 101
     304 194 197 275 111   8  98  73 307 156 161 340  38 183 110 135 192 177
     165 166 175 242 149 446 182 213 225  15 295 181 112   4   2 203 202 218
     189 263 132 221 363 254 187 245] 
     --------------------------------------------------
    ID 
     ['1371057' '2093270' '2783726' ... '996739474' '998468785' '998947462'] 
     --------------------------------------------------
    RESPOND 
     [0 1] 
     --------------------------------------------------
    AGE 
     ['71' '53' '45' '32' '35' '43' '39' '66' 45 '52' '29' '48' '67' '44' '59'
     '47' '49' '36' '23' '58' '50' '63' '55' '54' '42' '46' '60' '56' '37'
     '41' '34' '38' '33' '27' '68' '31' '19' '64' '40' '51' '28' '57' '18'
     '26' '21' '61' '20' '62' '30' '65' '69' '25' '72' '73' '70' '22' '24'
     '75' '74'] 
     --------------------------------------------------
    INCOME 
     ['67' '72' '70' '56' '66' '48' '49' '64' '65' 48 '58' '40' '57' '33' '17'
     '71' '25' '51' '52' '38' '54' '29' '63' '61' '19' '35' '31' '20' '24'
     '45' '73' '30' '53' '41' '37' '43' '60' '44' '27' '34' '21' '69' '55'
     '59' '18' '68' '39' '74' '50' '82' '28' '42' '114' '76' '46' '36' '62'
     '84' '16' '47' '81' '32' '75' '79' '77' '78' '26' '83' '22' '15' '86'
     '23' '92' '87' '105' '88' '107' '90' '93' '99' '85' '101' '80' '89' '96'] 
     --------------------------------------------------
    SEX 
     ['M' 'F'] 
     --------------------------------------------------
    MARRIED 
     ['1' '0'] 
     --------------------------------------------------
    FICO 
     ['719' '751' '725' '684' '651' '691' '694' '659' '692' '707' '705' '693'
     '698' '713' '703' '680' '610' '670' '686' '733' '714' '732' '706' '635'
     '687' '654' '720' '688' '646' '734' '717' '676' '673' '709' '727' '724'
     '657' '678' '738' '729' '744' '675' '701' '681' '643' '658' '650' '740'
     '672' '690' 694 '708' '685' '742' '677' '716' '704' '697' '695' '710'
     '715' '739' '668' '632' '737' '748' '718' '722' '666' '721' '769' '642'
     '683' '669' '712' '649' '679' '770' '674' '689' '749' '735' '730' '728'
     '616' '700' '777' '665' '699' '741' '664' '633' '648' '711' '682' '637'
     '661' '798' '647' '745' '702' '645' '757' '726' '696' '746' '667' '640'
     '619' '639' '662' '631' '750' '652' '660' '736' '671' '638' '625' '663'
     '723' '615' '630' '656' '731' '763' '603' '758' '644' '653' '655' '622'
     '776' '760' '623' '617' '636' '634' '761' '762' '628' '607' '743' '754'
     '747' '767' '627' '606' '780' '755' '765' '624' '593' '759' '611' '768'
     '641' '618' '756' '621' '752' '753' '614' '596' '598' '766' '601' '629'
     '793' '626' '784' '773' '781' '604' '597' '602' '771' '605' '764' '779'
     '620' '589' '772' '599' '613' '591' '794' '594' '585' '775' '608' '612'
     '800' '782' '791' '577'] 
     --------------------------------------------------
    OWNHOME 
     ['0' '1'] 
     --------------------------------------------------
    LOC 
     ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'] 
     --------------------------------------------------
    CLIMATE 
     [10 20 30] 
     --------------------------------------------------
    BUY6 
     [1 0 2] 
     --------------------------------------------------
    BUY12 
     [1 0 2 3] 
     --------------------------------------------------
    BUY18 
     [1 0 2 3] 
     --------------------------------------------------
    BUY24 
     [ 318   83  265  448  161  250  194  446  214  198  216  118  226  145
      494  485  258  230  189  412  499  107  228  162   97  283  337  349
      180   76  256  351  128  513   66  495  113  223  325  246  404  147
      247  644  119  201  243  422  143  191  464  165  144  259  179  169
      109  424  206  146  237   64   92  359  612  188  235  417  410   84
      330  164   98   87  137  354  541  726  199  207  120  187  195  236
      151  339  310  204   65  124  425  173  316  434  211  104  205  572
      114  234   78  127  208  398  341  219   75  405  231  227  110  450
      429  177  220  311   63  595  160   62  182  154  209  475  409  476
      455  108  239   81   95  121  126  213  232   96  253  163  386  251
      728  101  303   71  297  132   88  493  245  203  249   99  184  133
      487  241  218  700  326  200  210  197  225   74  252  175  150  319
      431  185  459  365  171  157  142   93  186  481  254  131  248  331
      229   82  720  159  115  238  620   89  407  103  396  138  282  302
      122  167  172  281  178  183  355  217  222  174  190  347  531  135
      421   67  443  148   77  156  152  334  123  463  141   85  299  202
      192  435  300   60  224   69  278  884  280  442  324  193  102   73
      290  437  263  385  567  168  212  515  460  451  356   94  155  401
      509  242  136   68   72  221  491  403  130  125  105  314  320  390
      470  158  149  257  321  170  377  129  112  176  483  166 1253  255
      261  284  333  139  556  275  420  360  373  591  602  387  504  244
       70  106  116  262  181  111  478  416  438  370  503  345  497  766
      399  279  489  117  433  346  306   80  240  379  338  472   90  196
      465  233  348  289  484  332  488  524  593  140  550  635   61  292
      376  153  419  313  389  384  288  371  492  364  134  308  327  471
      294  519  445  468  469  393  352  636  452  388  394  453  505   79
      312  268  606  276  941  100  436  456  565  362  562  739  415  328
      672  583  512  653  458  363  517  427  423  508  391  270  350  368
      291   91  215  507  729  756  426  260  634   86  599  500  554  498
      502  757  413  266  441  506  721  552  397  516  755  382  361  466
      820  704  406  690  428  601  418  477  395  746  486  375  329  286
      315  699  411  317  430  367  378  335  444  691  694  697  277  307
      722  578  273  701  340  626  344  447  304  462  343  366  540  696
      771  457  614  440  380  482  769  369  272  661  772  402  305  685
      473  322  323  751  545  358  353  600  555  589  414  267  790  480
      543  905  264  548  521  569  536  782  432  768  269  703  479  534
      566  518  838  621  686  285  730  539  525  392  847  301  496  792
      532  732  563  542  274  551  342  559  538  736  408  520  374  400
      295  535  695  684  588  986  622  725  747  514  647  474  381  727
      598  718  658  309  383  760  454  461  611  714  439  659  616  577
      372  298  640  510  287  523  336  683  557  675  296  529  271  608
      776  638  743 1026  579  698  627  357  811  490  560  568  511  677
      762  449  934  780  625  293  775  916  549  716  765  533  609  581
      617  742  501  774  576  666  922  607  467  909  741  715  682  645
      652  657  748  753  954  709  624  723  530  706  547  573  674  587
      527  788  798  575  630  662  860  605  597  561  604 1238  998  613
      680  926  892  667  564  843  977  596  528  927  787  877  885  584
     1012  670  603  669  952  930  781  574 1112  802  660  982  807  544
      580  869  917  553  649 1008  939  637  623  650  676  713  901  702
      761  724  663  646  984  758  975  858  619  968  628  793  770  794
      980  641  618  814  983  522 1204  991  655  878  754  678  671  914
     1010  639  801  933 1018  673  633  783  615  871  537  844  985  651
      656  919  654  863  832 1013  719  900  546  594 1017  915  935  668
      570  759  840  693  586] 
     --------------------------------------------------
    ORGSRC 
     ['O' 'R' 'D' 'C' 'U' 'P' 'I'] 
     --------------------------------------------------
    DISCBUY 
     [1 0] 
     --------------------------------------------------
    RETURN24 
     [0 1] 
     --------------------------------------------------
    COA6 
     [0 1] 
     --------------------------------------------------
    C1 
     [ 0 10  7  1 13  6  3  9 24 18  8 11  4 15 46 22 16  5  2 25 17 12 30 14
     29] 
     --------------------------------------------------
    C2 
     [  0  11  23   9  32  40  13  29   8  16  49  57  17  42  21  34  31  26
      73  37  33  10  24  12  53  39  22  20  28  47  18  19  38  80  35  14
      50  27  30  36  55  25  46  41  66  60  44  59  54  71  43  52  64  63
      15  92 101   5  45 115 111  93   4   2  61  67  51  65  77  90 112] 
     --------------------------------------------------
    C3 
     [  0  53   9  42  32  25  29  21  10  14   8  15  36  34  17  11  18  16
      30  13  31  41  39  12   5  35  19  68  22  20  54  28  38  27  24  23
      43  40  51  46  61  50  58  79  26   6  45  57 127  85  47  37  83] 
     --------------------------------------------------
    C4 
     [  0  40   3  82   9  31  41  26  14  27  11  13  28  17  37  21  19  35
      51  22  44  16   8  10  39   6  56  72  62  20  55  15  50  32  71  29
      12  38  23  24  18  53  36  33  30   7  49  25  85  54  73  48  34  59
      63 125  64 102  57] 
     --------------------------------------------------
    C5 
     [ 0 41  8 20 21 16 13 11 26 23  9 18 10 15 17 19 14 42 12 44 25 27 31 35
     29 33 37 55  3 38 54 90 24 56 22 58] 
     --------------------------------------------------
    C6 
     [  0  64  66  85  30  18  32  52  80  11  16  13  51  31 112   9  34  39
      45  72  19  35  28  17   8  77  21  25  22  23  38  47  65  12  55 188
      26  42  49  62  41  54  36 249  40 121  70  56 100  69  83 107 106  15
      29  37  27  57  48  61  50 117  58  14  24  68  82 128  73  76  93  79
      96  20 199  10 120  87 150  33  71  86 102  67  89 110 126  97  59  43
     136  46  81  44  78  53 103 118  84 129  74  75 158  88 135  63 104  99
      60 116   7  92 191 111 114 220  98 162 176 152 130  94 168 163 190 123
     153 105 124 101 139 115 149 147] 
     --------------------------------------------------
    C7 
     [ 0 11 19  8 10 20 15 13 38  9 12 16 14  1 17 41 22 32 62 30 21 24 25 56
     23 26 18] 
     --------------------------------------------------
    PURCHTOT 
     [  0  86 210 128  30  18  32   3 196   9  88  11 109  85  52 117  71 122
      23  34  39  81 102  60  64  65  26 186  21  25  51  49  41 116  58  12
      53 214  87  89 152  50 140 138  84 346 125  78  22 142  99  36  66 134
      79 114  45 115 229 147  67  37  44  61  42  70  83 150   6  93  31  33
     178 126  76  14  35  46  80  68  91 153  54  95  17  94  19 105  16 124
      27 108 151  62  20  63  43 113 163  47 288  10 118 160 127   5 171  90
      40  56 104  69  24 167 184 137 256 131  57 121 168  29  92 136 107  82
       1 208 106 129  72  96 174  97  48 123 145  77 148 173 139  13  74  75
     130 100 119 206 155 103 201 141 143 157   7 180 217 199  59  55 158 101
     304 194 197 275 111   8  98  73 307 156 161 340  38 183 110 135 192 177
     165 166 175 242 149 446 182 213 225  15 295 181 112   4   2 203 202 218
     189 263 132 221 363 254 187 245] 
     --------------------------------------------------
    


```python
df1['AGE'] = df1['AGE'].astype('int')
df1['INCOME'] = df1['INCOME'].astype('int')
df1['MARRIED'] = df1['MARRIED'].astype('int')
df1['OWNHOME'] = df1['OWNHOME'].astype('int')
df1['AGE'] = df1['AGE'].astype('int')

cate_cols = [col for col in df1.columns if df1[col].dtype == 'object']
num_cols =[col for col in df1.columns if df1[col].dtype in ['float', 'int64']]
cate_cols += ['CLIMATE','DISCBUY','RETURN24', 'COA6']




```


```python
#5.변수간 상관관계 확인
plt.figure(figsize= (15,15))
sns.heatmap(df1.corr(),annot=True, cmap='Blues')

```




    <matplotlib.axes._subplots.AxesSubplot at 0x1d9f138ff98>




![png](output_10_1.png)



```python
for col in cate_cols:
    if col not in ['RESPOND', 'ID']:
        pd.crosstab(df1[col],df1['RESPOND']).plot.bar()
        plt.show  

        
        

```


![png](output_11_0.png)



![png](output_11_1.png)



![png](output_11_2.png)



![png](output_11_3.png)



![png](output_11_4.png)



![png](output_11_5.png)



![png](output_11_6.png)



```python
#6.Training data 균형 맞추기-> 
#목표변수 (Respond) 값 0과 1의 비율이 92:7 (=13:1) 로, 결과가 편향될 수 있음 -> SMOTE를 통한 데이터 불균형처리
from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
!pip install imblearn
from imblearn.over_sampling import SMOTE
```

    Requirement already satisfied: imblearn in c:\programdata\anaconda3\lib\site-packages (0.0)
    Requirement already satisfied: imbalanced-learn in c:\programdata\anaconda3\lib\site-packages (from imblearn) (0.4.3)
    Requirement already satisfied: scikit-learn>=0.20 in c:\programdata\anaconda3\lib\site-packages (from imbalanced-learn->imblearn) (0.20.3)
    Requirement already satisfied: scipy>=0.13.3 in c:\programdata\anaconda3\lib\site-packages (from imbalanced-learn->imblearn) (1.2.1)
    Requirement already satisfied: numpy>=1.8.2 in c:\programdata\anaconda3\lib\site-packages (from imbalanced-learn->imblearn) (1.16.2)
    


```python
x=pd.get_dummies(df1.iloc[:,1:], drop_first=True)


pd.set_option('display.max_columns', 500)

```


```python
sm = SMOTE(ratio='auto', kind='regular')
x_resampled, y_resampled = sm.fit_sample(x, x['RESPOND'])
x_resampled = pd.DataFrame(x_resampled)
x_resampled.columns = x.columns

# smote로 샘플링한 후 데이터 프레임으로 변경했고, 컬럼명을 불러다 지정해줬다.
```

### SMOTE의 개념


 불균형 자료의 경우 모델을 편향되게 할 수 있다. 예를 들어, 모든 인풋에 대해 결과값이 0인 모델을 만들게 되는데 그렇게 되도 정확도는 높게 나오기 때문이 그렇기 때문에 불균형 자료에 대한 사전 처리가 필요하다. 다운샘플링의 경우 자료의 손실이 많고, 오버샘플링의 경우 과적합의 문제가 있을 수 있다. 그래서 단순 오버샘플링 대신 SMOTE 알고리즘을 이용하여 불균형 자료를 처리하는 방법이 많이 사용되고 있다.
 

> smote가 사용하는 방법은 단순하다. 적은 수의 클래스의 데이터 포인트에서 근접이웃들을 찾고, 각 데이터 포인트와의 사이에 선을 긋는다. 그 선상에서 임의의 점을 생성한다.


![title](https://cdn-images-1.medium.com/max/1600/1*6UFpLFl59O9e3e38ffTXJQ.png)

위의 이미지를 보면 5개의 근접이웃을 찾고 연결하여 그 선 위의 임의의 점을 선택하여 샘플을 생성한다.


### 의사결정 나무(Decision Tree)

의사 결정 나무는 분기 때마다 변수의 영역을 나누는 모델이다. 그 기준은 __순도(Homegeneity)__ 가 증가하고, __불순도(Impurity)__ 또는 불확실성이 감소하는 방향으로 진행되는데 이걸  __정보획득(Information Gain)__ 이라고 한다.  

#### 분류 기준이 되는 지표
어떤 데이터가 이 조건에 충족하는지 보기 위한 지표는 대표적으로 3가지가 있다  
<br/>
* __엔트로피(Entropy)__
 <br/>
$$ Entropy(A) = \sum_ {i=1}^d R_i\Bigl( - \sum_ {k=1}^m log_ {2}(P_k) \Bigl)  $$
 ($P_k$ = A영역에 속한 레코드 중 k범주에 속하는 레코드의 비율, $R_i$=분할 전 레코드 중 분할 후 i영역에 속하는 레코드의 비율)
 
 <br/>
불확실성인 엔트로피가 최대한 감소하는 모델을 찾는다. 이 때 엔트로피는 A의 모든 레코드가 동일한 범주에 속할 경우 0이 되며 범주가 두 개일때 반반씩 섞여있을 경우 0.5로 최대가 된다.
<br/> 
<br/>
$$ Gain(A) = I(s_1,s_2,...,s_m) - E(A) $$
($ s_1,s_2,...,s_m $ 은 상위 노드의 Entropy, $ E(A) $는 하위 각 노드의 엔트로피를 계산 한 후 노드의 속한 레코드의 개수를 가중치로 하여 엔트로피를 평균한 값)


    * Information Gain: 지정된 속성이 얼마나 잘 training example들간을 구분하는가에 대한 수치.
    * Entropy: example들의 집합에서의 혼합성(impurity)을 나타냄
<br/>
<br/>
* __지니계수(Gini Index) __  
<br/>
$$ G.I(A) = \sum_ {i=1}^d \Bigl( R_i \Bigl(1- \sum_ {k=1}^m P_ {ik}^2 \Bigl) \Bigl) $$
<br/>
<br/>
<br/>
* 다른 한가지인 오분류오차의 경우 미분이 불가능하기 때문에 잘 쓰이지 않는다.
<br/>  

![title](http://i.imgur.com/n3MVwHW.png)  

<br/>
<br/>
<br/>

#### 분류 과정
의사결정 나무의 모델링 방식은 처음에 한 변수를 선택하여 크기로 오더링 하고 첫번째 레코드와 나머지 레코드들로 분류해 엔트로피를 구하고, 첫번째와 두번째 레코드와 나머지로 분류해 레코드를 구하는 방식으로 가능한 모든 분기의 엔트로피를 구한 뒤 다른 변수들에 대해서도 같은 과정을 반복한다. 그 중에서 가장 엔트로피가 많이 감소하는 분기를 택해 분류한다. 이와 같은 방법을 시행하면 계산해야 하는 식의 수는 $d(n-1)$이 된다.   

위의 방식으로 분기를 계속하면 분기가 진행됨에 따라 오분류율이 감소하다가 다시 증가하는 점이 오는데 이러한 과적합을 방지하기 위해 __가지치기(Pruning)__ 를 진행한다. terminal node의 순도가 100%가 된 상태를 __Full Tree__ 라고 하는데, 적정선에서 가지들끼리 merge해 주는 것을 말한다.

<br/>
<br/>  

#### 가지치기
* __가지치기의 비용함수__    
$$ CC(T) = ERR(T)  +  \alpha \times L(T) $$

$CC(T)$=의사결정나무의 비용 복잡도(=오류가 적으면서 terminal node가 적은 단순한 모델일 수록 작은 값)  
$ERR(T)$=검증데이터에 대한 오분류율   
$L(T)$=terminal node의 수(구조의 복잡도)  
$\alpha=ERR(T)$와 $L(T)$를 결합하는 가중치(사용자에 의해 부여됨, 보통 0.01~0.1의 값을 씀)  

<br/><br/>
#### 의사결정 나무의 분류

|알고리즘|평가 기준(지수)|분류방법
|--------|------------------|------------------------------|
|ID3|Entropy|다지분리|
|C 4.5|Information Gain|다지분리(범주) 및 이진분리(수치)|
|C 5.0|Information Gain|다지분리 및 이진분리|
|CHAID|카이제곱(범주), F검정(수치)|통계적 접근방식|
|CART|Gini Index(범주), 분산의 차이(수치)|통계적 접근방식, 항상 이진|

>C4.5는 속성이 갖는 범주값의 수만큼 분리를 하기 때문에 실 데이터의 분석에서 가지가 매우 잘게 나눠지는 문제가 있다. 반면, CART는 딱 2개로만 분리하기 때문에 지나치게 단순화되는 문제가 발생할 수 있다


<br/>
<br/>  

#### 의사결정 나무에 대하여
의사결정 나무는 계산복잡성 대비 성능이 좋고, 변수에 대한 설명이 용이하다는 장점이 있다. 하지만 __결정경계(Decision Boundary)__ 가 데이터축에 수직이기 때문에 비선형인 데이터에 대해선 적합하지 않은 단점이 있고 이를 해결하기 위해 등장한 것이 __Random Forest__ 이다.



```python
#8. Entropy Decision Tree(ID3) 생성   
#9. Gini Index Decision Tree(CART) 생성

from sklearn.tree import DecisionTreeClassifier,export_graphviz # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

y = x_resampled['RESPOND']
x_train, x_test, y_train, y_test = train_test_split(x_resampled, y, test_size=0.2, random_state=1)
entropy = DecisionTreeClassifier(criterion='entropy', random_state=1)
gini = DecisionTreeClassifier(criterion='gini', random_state=1)
ent_fit = entropy.fit(x_train, y_train)
gini_fit = gini.fit(x_train, y_train)
'''
random_state : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)
shuffle : 셔플여부설정 (default = True)
stratify : 지정한 Data의 비율을 유지한다. 예를 들어, Y가 3:1의 두 클래스이면, stratify=Y로 설정하면 데이터셋들도 3:1.
'''
```




    '\nrandom_state : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)\nshuffle : 셔플여부설정 (default = True)\nstratify : 지정한 Data의 비율을 유지한다. 예를 들어, Y가 3:1의 두 클래스이면, stratify=Y로 설정하면 데이터셋들도 3:1.\n'




```python
from sklearn.metrics import accuracy_score
#10.두 모델의 비교(개념 공부해보기)
y_ent_pred = ent_fit.predict(x_test)
y_gini_pred = gini_fit.predict(x_test)
print(' entropy score:%.2f'% accuracy_score(y_test, y_ent_pred),'\n',
      'gini score:%.2f'%accuracy_score(y_test, y_gini_pred))

## 왜이렇게 트레인이 잘 됐지
```

     entropy score:1.00 
     gini score:1.00
    


```python
import os
os.environ['PATH'] += os.pathsep + 'C:\Program Files (x86)\Graphviz2.38\bin'
!pip install graphviz
!pip install pydot
import io
import  pydotplus
from IPython.core.display import Image
from sklearn.tree import export_graphviz
```

    Requirement already satisfied: graphviz in c:\programdata\anaconda3\lib\site-packages (0.11)
    Requirement already satisfied: pydot in c:\programdata\anaconda3\lib\site-packages (1.4.1)
    Requirement already satisfied: pyparsing>=2.1.4 in c:\programdata\anaconda3\lib\site-packages (from pydot) (2.3.1)
    


```python
dot_data = export_graphviz(ent_fit, out_file=None, feature_names=x_resampled.columns,  
                           class_names=["resoponse", "none"],
                           filled=True, rounded=True, special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())
```




![png](output_20_0.png)




```python
dot_data = export_graphviz(gini_fit, out_file=None, feature_names=x_resampled.columns,  
                           class_names=["resoponse", "none"],
                           filled=True, rounded=True, special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())
```




![png](output_21_0.png)




```python
##################################################################################3
#1.	데이터 불러오기 (CallsData.xls&ContractData.csv) 
df2 = pd.read_excel('C:/Users/kyuri Im/Desktop/bigdata/Data_Study_19.05.19~/20190526 과제/20190526 과제/CallsData.xls',encoding='utf-8')   
df3 = pd.read_csv('C:/Users/kyuri Im/Desktop/bigdata/Data_Study_19.05.19~/20190526 과제/20190526 과제/ContractData.csv') 
```


```python
#2.	데이터 합치기
df = df2.merge(df3, on=['Area Code','Phone'], how='left')
```


```python
#3.	데이터 검토 및 통계분석
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VMail Message</th>
      <th>Day Mins</th>
      <th>Eve Mins</th>
      <th>Night Mins</th>
      <th>Intl Mins</th>
      <th>CustServ Calls</th>
      <th>Day Calls</th>
      <th>Day Charge</th>
      <th>Eve Calls</th>
      <th>Eve Charge</th>
      <th>Night Calls</th>
      <th>Night Charge</th>
      <th>Intl Calls</th>
      <th>Intl Charge</th>
      <th>Area Code</th>
      <th>Phone</th>
      <th>Account Length</th>
      <th>Churn</th>
      <th>Int'l Plan</th>
      <th>VMail Plan</th>
      <th>State</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25</td>
      <td>265.1</td>
      <td>197.4</td>
      <td>244.7</td>
      <td>10.0</td>
      <td>1</td>
      <td>110</td>
      <td>45.07</td>
      <td>99</td>
      <td>16.78</td>
      <td>91</td>
      <td>11.01</td>
      <td>3</td>
      <td>2.70</td>
      <td>415</td>
      <td>382-4657</td>
      <td>128</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>KS</td>
    </tr>
    <tr>
      <th>1</th>
      <td>26</td>
      <td>161.6</td>
      <td>195.5</td>
      <td>254.4</td>
      <td>13.7</td>
      <td>1</td>
      <td>123</td>
      <td>27.47</td>
      <td>103</td>
      <td>16.62</td>
      <td>103</td>
      <td>11.45</td>
      <td>3</td>
      <td>3.70</td>
      <td>415</td>
      <td>371-7191</td>
      <td>107</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>OH</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>243.4</td>
      <td>121.2</td>
      <td>162.6</td>
      <td>12.2</td>
      <td>0</td>
      <td>114</td>
      <td>41.38</td>
      <td>110</td>
      <td>10.30</td>
      <td>104</td>
      <td>7.32</td>
      <td>5</td>
      <td>3.29</td>
      <td>415</td>
      <td>358-1921</td>
      <td>137</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NJ</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>299.4</td>
      <td>61.9</td>
      <td>196.9</td>
      <td>6.6</td>
      <td>2</td>
      <td>71</td>
      <td>50.90</td>
      <td>88</td>
      <td>5.26</td>
      <td>89</td>
      <td>8.86</td>
      <td>7</td>
      <td>1.78</td>
      <td>408</td>
      <td>375-9999</td>
      <td>84</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>OH</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>166.7</td>
      <td>148.3</td>
      <td>186.9</td>
      <td>10.1</td>
      <td>3</td>
      <td>113</td>
      <td>28.34</td>
      <td>122</td>
      <td>12.61</td>
      <td>121</td>
      <td>8.41</td>
      <td>3</td>
      <td>2.73</td>
      <td>415</td>
      <td>330-6626</td>
      <td>75</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>OK</td>
    </tr>
  </tbody>
</table>
</div>




```python
#4.	모델에 사용되지 않을 변수 제거(Phone, Account Length) 
del df['Phone']
del df['Account Length']
```


```python
#5.	명목형 변수의 경우 수치형 일 때 -> String으로 변환-
```


```python
#6.	변수 표준화 (Normalizaion)
for i in range(0,15):
    df.iloc[:,i]=(df.iloc[:,i]-df.iloc[:,i].min())/(df.iloc[:,i].max()-df.iloc[:,i].min())
  
```


```python
#7.	학습용 데이터와 테스트 데이터 80:20으로 분리
from sklearn.tree import DecisionTreeClassifier,export_graphviz # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
import pydot
X = 
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)
'''
random_state : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)
shuffle : 셔플여부설정 (default = True)
stratify : 지정한 Data의 비율을 유지한다. 예를 들어, Y가 3:1의 두 클래스이면, stratify=Y로 설정하면 데이터셋들도 3:1.

'''

#8.	SMOTE로 목표변수(churn) 0과 1의 비율 맞추기
#9.	Gain Ratio Decision Tree 생성 
#10.Gini Index Decision Tree 생성
#11.두 모델의 비교
```


      File "<ipython-input-26-1111fb3e3d3a>", line 6
        X =
            ^
    SyntaxError: invalid syntax
    



```python


```


```python

```
